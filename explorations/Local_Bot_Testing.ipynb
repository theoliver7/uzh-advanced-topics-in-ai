{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:15:50.246441300Z",
     "start_time": "2023-11-25T13:15:50.241476800Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/oliver/dev/uzh/atai_bot/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from bot.bot import Bot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:15:53.598476600Z",
     "start_time": "2023-11-25T13:15:50.247953400Z"
    }
   },
   "id": "6683d2d02c573fea"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1650, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/oliver/dev/uzh/atai_bot/mistral-7b-openorca.Q4_0.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32002,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32002,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name   = open-orca_mistral-7b-openorca\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 32000 '<dummy32000>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  = 1343.28 MB\n",
      "llm_load_tensors: offloading 22 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 22/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 2574.69 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1524\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  190.50 MB\n",
      "llama_new_context_with_model: compute buffer total size = 187.79 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 181.91 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: total VRAM used: 2756.60 MB (model: 2574.69 MB, context: 181.91 MB)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--STARTING TO LOAD GRAPH--\n",
      "--LOADED GRAPH--\n",
      "--LOADED MOVIE DICT--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---READY FOR OPERATION---\n"
     ]
    }
   ],
   "source": [
    "bot = Bot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:16:32.930043600Z",
     "start_time": "2023-11-25T13:15:53.606312Z"
    }
   },
   "id": "a6bb6d4f884a79bc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "questions = [\"Is Public Enemy No. 1 - Todestrieb a crime thriller?\",\n",
    "             \"Did Christopher Nolan direct Inception?\",\n",
    "             \"Is GoldenEye 007 a James Bond movie?\",\n",
    "             \"Is Following a black and white film?\",\n",
    "             \"Does the The Lord of the Rings trilogy consist of three movies?\",\n",
    "             \"Does First Man depict the life of Neil Armstrong?\",\n",
    "             \"Is La Princesse de Cl√®ves set in the French Renaissance period?\",\n",
    "             \"Is 2001: A Space Odyssey directed by Stanley Kubrick?\",\n",
    "             \"Is Devil in the Flesh 2  a sequel?\",\n",
    "             \"Did James Cameron direct Titanic?\",\n",
    "             \"Is The Godfather based on a novel by Mario Puzo?\",\n",
    "             \"Is The Masked Gang: Cyprus a Turkish comedy film?\",\n",
    "             \"Does Star Wars: Episode VI - Return of the Jedi feature the character Luke Skywalker?\",\n",
    "             \"Who directed the movie Apocalypse Now?\",\n",
    "             \"What movie won the Best Picture Oscar in 2021?\",\n",
    "             \"Which film features a character named Sarah Connor?\",\n",
    "             \"Can you list three movies directed by Martin Scorsese?\",\n",
    "             \"What is the highest-grossing film of all time?\",\n",
    "             \"Who composed the score for the 'Pirates of the Caribbean' series?\",\n",
    "             \"Name a science fiction movie with a twist ending.\",\n",
    "             \"What's the name of the fictional land in 'The Lord of the Rings'?\",\n",
    "             \"Who played the role of Harry Potter in the movie series?\",\n",
    "             \"What is the name of the ship in 'Titanic'?\",\n",
    "             \"In which movie did Tom Hanks say, 'Life is like a box of chocolates'?\",\n",
    "             \"Which actor has played the role of Spider-Man on film?\",\n",
    "             \"Name a movie where Meryl Streep plays a chef.\",\n",
    "             \"Who starred as the leading actor in 'The Revenant'?\",\n",
    "             \"Can you list three films Jennifer Lawrence has been in?\",\n",
    "             \"What was Will Smith's first movie?\",\n",
    "             \"Who directed 'The Grand Budapest Hotel'?\",\n",
    "             \"Name a movie directed by Ava DuVernay.\",\n",
    "             \"Which director is known for the 'Dark Knight' trilogy?\",\n",
    "             \"Who directed 'Schindler's List' and what year was it released?\",\n",
    "             \"Can you name a horror movie directed by Jordan Peele?\",\n",
    "             \"What is the IMDb rating for 'The Shawshank Redemption'?\",\n",
    "             \"Which movie has a perfect score on Rotten Tomatoes?\",\n",
    "             \"Can you give me the Metacritic score for 'Mad Max: Fury Road'?\",\n",
    "             \"What is the parental rating for 'Jurassic Park'?\",\n",
    "             \"How many stars did 'La La Land' receive in its original New York Times review?\",\n",
    "             \"Can you show me a positive review for 'The Godfather'?\",\n",
    "             \"What did critics say about 'Avatar'?\",\n",
    "             \"Are there any Oscar-winning movies that received bad reviews?\",\n",
    "             \"What was the general consensus on 'The Shape of Water' after its release?\",\n",
    "             \"Did 'Black Panther' get good reviews?\",\n",
    "             \"What 1994 crime film revitalized John Travolta's career?\",\n",
    "             \"Which movie's famous line is 'I'll be back'?\",\n",
    "             \"What is the fictional brand of cigarettes in Quentin Tarantino's films?\",\n",
    "             \"Can you name a movie where the protagonist is a toy?\",\n",
    "             \"Which film features an iconic dance scene between Uma Thurman and John Travolta?\",\n",
    "             \"Which movies are part of the Marvel Cinematic Universe Phase 4?\",\n",
    "             \"Name a movie that has been remade more than once.\",\n",
    "             \"What is the theme song of 'Mission: Impossible'?\",\n",
    "             \"Who voiced Woody in 'Toy Story'?\",\n",
    "             \"What are the three rules for caring for a Mogwai in 'Gremlins'?\",\n",
    "             'What is the genre of Shoplifters?',\n",
    "             'Who directed Incepiton?',\n",
    "             'When was the dark knight released?',\n",
    "             'Was Captain America Cilivl War released in 2008?',\n",
    "             \"Who is the director of the Batman movie?\",\n",
    "             \"Did Christopher Nolan ever work on a Batman movie?\",\n",
    "             \"What is the name of the lead actor in the movie Catch Me If You Can?\",\n",
    "             ]\n",
    "\n",
    "rec_questions = [\"I like the Jurassic Park movie; can you recommend any similar movies?\",\n",
    "                 \"I am a big fan of Steven Spielberg, could you recommend some of his action movies?\",\n",
    "                 \"What are movies to watch on a rainy day?\",\n",
    "                 \"What should I watch after Titanic?\",\n",
    "                 \"I don't like horror movies, what should I watch?\",\n",
    "                 \"Recommend movies similar to Hamlet and Othello.\",\n",
    "                 \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\",\n",
    "                 \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\",\n",
    "                 \"What do you recommend to watch after inside out?\",\n",
    "                 \"Any suggestion for movies to watch with my girlfriend?\",\n",
    "                 ]\n",
    "\n",
    "off_topic_questions = ['Are you a human being?', 'Who develop you?', 'Who are you?', 'Where is Zurich?',\n",
    "                       'Who is president of Switzerland?', 'What do you think of this class?',\n",
    "                       'Was Captain America Cilivl War released in 2008?']\n",
    "\n",
    "multimedia = [\n",
    "    \"Show me the pictures of the lead actors of the movie Jurassic Park.\",\n",
    "    \"Can you show me the poster of the movie Batman?\",\n",
    "    \"Show me an action movie poster.\"\n",
    "]\n",
    "\n",
    "crowd_sourcing= [\n",
    "    \"What is the box office of The Princess and the Frog? \",\n",
    "    \"Can you tell me the publication date of Tom Meets Zizou? \",\n",
    "    \"Who is the executive producer of X-Men: First Class? \"\n",
    "]\n",
    "olat_questions = [\n",
    "    \"Who is the director of Good Will Hunting? \",\n",
    "    \"Who directed The Bridge on the River Kwai? \",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi? \",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus? \",\n",
    "    \"When was The Godfather released? \",\n",
    "    \"What is the MPAA film rating of Weathering with You? \",\n",
    "    \"What is the genre of Good Neighbors? \",\n",
    "    \"Show me a picture of Halle Berry. \",\n",
    "    \"What does Julia Roberts look like? \",\n",
    "    \"Let me know what Sandra Bullock looks like. \",\n",
    "    \"Recommend movies similar to Hamlet and Othello. \",\n",
    "    \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies? \",\n",
    "    \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween. \",\n",
    "    \"What is the box office of The Princess and the Frog? \",\n",
    "    \"Can you tell me the publication date of Tom Meets Zizou? \",\n",
    "    \"Who is the executive producer of X-Men: First Class? \",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:16:32.930043600Z",
     "start_time": "2023-11-25T13:16:32.889194800Z"
    }
   },
   "id": "2b6ec124d3f9dd1c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f26fb7c69933d87f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the box office of The Princess and the Frog? \n",
      "_____________________\n",
      "People: []\n",
      "Movies: ['The Princess and the Frog']\n",
      "FUZZYWUZZY results: ('the princess and the frog', 65.78947368421053, 24358)\n",
      "FUZZYWUZZY results: ('the princess and the frog', 100.0, 24358)\n",
      "node label:The Princess and the Frog\n",
      "tag:entertaining, fantasy, romantic, thought-provoking\n",
      "node description:2009 American animated musical romantic fantasy film by Disney\n",
      "film editor:Jeff Draheim\n",
      "genre:fantasy film, traditionally animated film, buddy film, musical film, comedy-drama, romantic comedy\n",
      "nominated for:Academy Award for Best Animated Feature\n",
      "executive producer:John Lasseter, Monica Lago-Kaytis\n",
      "based on:The Frog Prince\n",
      "performer:Randy Newman\n",
      "part of the series:Walt Disney Animation Studios film\n",
      "depicts:The Frog Prince, metamorphosis, frog, Mardi Gras, Louisiana Voodoo, princess\n",
      "FSK film rating:FSK 0\n",
      "fabrication method:traditional animation\n",
      "set in period:1920s, 1910s\n",
      "production designer:James Aaron Finch\n",
      "production company:Walt Disney Pictures\n",
      "instance of:animated feature film\n",
      "art director:Mike Gabriel, Ian Gooding\n",
      "storyboard artist:Don Hall, Paul Briggs, Toby Shelton, Tom Ellery, Josie Trinidad, Bruce Morris, Kevin Gollaher, Randy Cartwright, Wilbert Plijnaar\n",
      "musical conductor:Randy Newman\n",
      "IMDb ID:tt0780521\n",
      "original language of film or TV show:English\n",
      "distribution format:video on demand\n",
      "color:color\n",
      "country of origin:United States of America\n",
      "IMDA rating:PG\n",
      "director:John Musker, Ron Clements\n",
      "publication date:2009-11-25\n",
      "screenwriter:Greg Erb, Jason Oremland, Rob Edwards, John Musker, Ron Clements\n",
      "characters:Prince Naveen, Dr. Facilier, Louis, Charlotte La Bouff, Ray, Mama Odie, Juju, Eudora, James, Eli La Bouff, Lawrence, Reggie, Darnell and Two Fingers, Mob Shadows, Tiana\n",
      "animator:Dean Wellins, Ted Kierscey, Dale Baer, Mark Henn, Jin Kim, Duncan Marjoribanks, Michael Surrey, T. Daniel Hofstedt, Randy Haycock, James Lopez, Brian Ferguson, Alex Kupershmidt, Sandro Cleuzo, Yoshimichi Tamura, Marlon West, Michael Show, Jared Beckstrand, Sam Marin, Richard Hoppe, Mark Myer, Tim Allen, Allen Blyth, Mauro Maressa, David Mildenberger, Garrett Wren, Eric Walls, Bill Waldman, James DeValera Mansfield, Danny Galieote, Dan Lund, Andreas Wessel-Therhorn, Bert Klein, Joe Oh, Adam Dykstra, Phillip Vigil, Bob Bennett, Tony West, Masa Oshiro, Bob Davies, Roberto Casale, Frans Vischer, Hyun Min Lee, Matt William√™s, Enoc Castaneda Jr., Dan Turner, Bruce W. Smith, Anthony DeRosa, Eric Goldberg, Andreas Deja, Nik Ranieri, Pres Romanillos, Randy Cartwright, Ruben A. Aquino, Russ Edmonds\n",
      "voice actor:Kimberly Russell, Mick Wingert, John Kassir, Peter Del Vecho, Peter Bartlett, Don Hall, Paul Briggs, David Cowgill, John Goodman, Jerry Kernion, Terri Douglas, Anika Noni Rose, Mona Marshall, Jeff Draheim, Marlon West, Elizabeth Dampier, Breanna Brooks, Ritchie Montgomery, Danielle Mon√© Truitt, Bruce W. Smith, Terrence Howard, Jim Cummings, Randy Newman, Peter Renaday, Philip Proctor, Emeril Lagasse, Keith David, Rif Hutton, Eddie Frierson, Roger Aaron Brown, Kevin Michael Richardson, Bridget Hoffman, Jenifer Lewis, Oprah Winfrey, Bruno Campos, Jennifer Cody, Joe Whyte, Kwesi Boakye, Michael-Leon Wooley, Michael Colyar, John Musker, Fred Tatasciore, Corey Burton\n",
      "has edition or translation:K√ºss den Frosch\n",
      "distributed by:F√≥rum Hungary, Walt Disney Studios Motion Pictures, Disney+, FandangoNow, Netflix\n",
      "significant event:premi√®re\n",
      "references work, tradition or theory:Pinocchio\n",
      "narrative location:New Orleans\n",
      "set in environment:bayou\n",
      "main subject:social structure, goal pursuit, fairy tale, jazz\n",
      "inspired by:The Frog Princess\n",
      "box office:267000000\n",
      "POSTING RESPONE:  The Princess and the Frog had a box office of $267 million.\n",
      "[Crowd, inter-rater agreement 0.236, The answer distribution for this specific task was 2 support votes, 1 reject votes]\n",
      "Took: 12.33710761400016 seconds,avg: 12.33710761400016\n",
      "Question: Can you tell me the publication date of Tom Meets Zizou? \n",
      "_____________________\n",
      "People: []\n",
      "Movies: ['Tom Meets Zizou']\n",
      "FUZZYWUZZY results: ('dawn of the planet of the apes', 51.764705882352935, 22749)\n",
      "FUZZYWUZZY results: ('tom meets zizou', 100.0, 2192)\n",
      "node label:Tom Meets Zizou\n",
      "node description:2011 film by Aljoscha Pause\n",
      "genre:documentary film\n",
      "cast member:Thomas Broich\n",
      "image:https://commons.wikimedia.org/wiki/File:Tom_meets_Zizou_‚Äì_Kein_Sommerm√§rchen.jpg\n",
      "instance of:film\n",
      "IMDb ID:tt1998364\n",
      "original language of film or TV show:German\n",
      "color:color\n",
      "country of origin:Germany\n",
      "director:Aljoscha Pause\n",
      "publication date:2011-01-01\n",
      "screenwriter:Aljoscha Pause\n",
      "sport:association football\n",
      "main subject:association football\n",
      "POSTING RESPONE:  The film \"Tom Meets Zizou\" was published on January 1, 2011. \n",
      "[Crowd, inter-rater agreement 0.04, The answer distribution for this specific task was 0 support votes, 3 reject votes]\n",
      "Took: 6.832296855000095 seconds,avg: 9.584702234500128\n",
      "Question: Who is the executive producer of X-Men: First Class? \n",
      "_____________________\n",
      "People: []\n",
      "Movies: ['X - Men : First Class']\n",
      "FUZZYWUZZY results: ('the keeper of lost causes', 52.63157894736843, 16909)\n",
      "FUZZYWUZZY results: ('x-men: first class', 92.3076923076923, 22902)\n",
      "node label:X-Men: First Class\n",
      "tag:action, cult, entertaining, fantasy, flashback, good_versus_evil, murder, psychedelic, revenge\n",
      "node description:2011 superhero film directed by Matthew Vaughn\n",
      "film editor:Eddie Hamilton\n",
      "genre:teen film, superhero film, fantasy film, action film, science fiction film\n",
      "named after:X-Men: First Class (comics)\n",
      "executive producer:Sheryl Lee Ralph\n",
      "takes place in fictional universe:Earth-10005\n",
      "based on:X-Men\n",
      "cast member:Corey Johnson, Aleksander Krupa, Hugh Jackman, James Faulkner, Matt Craven, Gene Farber, Jennifer Lawrence, James McAvoy, Rebecca Romijn, January Jones, Zo√´ Kravitz, Rose Byrne, Sasha Pieterse, Annabelle Wallis, Morgan Lily, Nicholas Hoult, Demetri Goritsas, Lucas Till, Edi Gathegi, √Ålex Gonz√°lez, Randall Batinkoff, Oliver Platt, Michael Ironside, Kevin Bacon, Neil Fingleton, Rade ≈†erbed≈æija, James Remar, Jason Flemyng, Ray Wise, Michael Fassbender, Glenn Morshower, Brendan Fehr, Tony Curran, Wilfried Hochholdinger, Bill Milner, Caleb Landry Jones, Don Creech, Jason Beghe, Ludger Pistor\n",
      "MPAA film rating:PG-13\n",
      "performer:Henry Jackman\n",
      "part of the series:X-Men, X-Men Beginnings\n",
      "FSK film rating:FSK 12\n",
      "aspect ratio:2.35:1\n",
      "box office:353624124\n",
      "set in period:1944\n",
      "costume designer:Sammy Sheldon\n",
      "production designer:Chris Seager\n",
      "production company:Marvel Entertainment, RatPac-Dune Entertainment, 20th Century Studios, Ingenious Media\n",
      "instance of:film\n",
      "director of photography:John Mathieson\n",
      "IMDb ID:tt1270798\n",
      "original language of film or TV show:English\n",
      "distribution format:theatrical release, video on demand\n",
      "color:color\n",
      "country of origin:United Kingdom, United States of America\n",
      "IMDA rating:PG\n",
      "director:Matthew Vaughn\n",
      "publication date:2011-05-25\n",
      "screenwriter:Matthew Vaughn, Jane Goldman, Ashley Miller, Zack Stentz\n",
      "distributed by:20th Century Studios, Disney+, FandangoNow, Netflix\n",
      "IFCO rating:12A\n",
      "narrative location:New York, Soviet Union, Cuba, Miami metropolitan area, Poland, Switzerland, Argentina, Washington, D.C., Moscow, London, Miami\n",
      "filming location:Georgia, Russia, Switzerland, Los Angeles, Pinewood Studios, London, California\n",
      "main subject:Cold War\n",
      "POSTING RESPONE:  The executive producer of X-Men: First Class is Sheryl Lee Ralph.\n",
      "[Crowd, inter-rater agreement 0.199, The answer distribution for this specific task was 2 support votes, 1 reject votes]\n",
      "Took: 8.389046448000045 seconds,avg: 9.186150305666766\n"
     ]
    }
   ],
   "source": [
    "for question in crowd_sourcing:\n",
    "    bot.ask(question)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:17:00.472812700Z",
     "start_time": "2023-11-25T13:16:32.897229800Z"
    }
   },
   "id": "c92425d321ada4d0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Avatar released?\n",
      "_____________________\n",
      "People: []\n",
      "Movies: ['Avatar']\n",
      "FUZZYWUZZY results: ('the master cleanse', 61.904761904761905, 7323)\n",
      "FUZZYWUZZY results: ('avatar', 100.0, 23097)\n",
      "node label:Avatar\n",
      "tag:action, allegory, boring, cruelty, dramatic, entertaining, fantasy, good_versus_evil, insanity, murder, psychedelic, revenge, romantic, sadist, sci-fi, suspenseful, thought-provoking, violence\n",
      "node description:2009 American epic science fiction film directed by James Cameron\n",
      "film editor:Stephen E. Rivkin, James Cameron, John Refoua\n",
      "genre:action film, science fiction, live-action/animated film, adventure film, science fiction film, military science fiction\n",
      "named after:avatar\n",
      "nominated for:Academy Award for Best Picture, Academy Award for Best Director, MTV Movie Award for Best Villain, Academy Award for Best Cinematography, Academy Award for Best Production Design, Academy Award for Best Film Editing, Academy Award for Best Visual Effects, Academy Award for Best Sound Editing, Academy Award for Best Original Score, Academy Award for Best Sound\n",
      "executive producer:Colin Wilson\n",
      "takes place in fictional universe:fictional universe of Avatar\n",
      "cast member:Sigourney Weaver, Scott Lawrence, Dileep Rao, Michelle Rodriguez, Matt Gerald, Zoe Saldana, Debra Wilson, Anthony Ingruber, Giovanni Ribisi, CCH Pounder, Laz Alonso, Stephen Lang, Wes Studi, Joel David Moore, Peter Mensah, Sam Worthington\n",
      "award received:Golden Globe Award for Best Motion Picture ‚Äì Drama, Saturn Award for Best Director, Academy Award for Best Cinematography, Academy Award for Best Production Design, Academy Award for Best Visual Effects, Golden Globe Award for Best Director\n",
      "performer:James Horner\n",
      "part of the series:Avatar\n",
      "image:https://commons.wikimedia.org/wiki/File:Amateur-made_Na'vi.jpg\n",
      "FSK film rating:FSK 12\n",
      "aspect ratio:2.39:1\n",
      "fabrication method:computer animation, motion capture\n",
      "box office:2744336793\n",
      "uses:MASSIVE, RenderMan\n",
      "production designer:Robert Stromberg, Rick Carter\n",
      "production company:RatPac-Dune Entertainment, 20th Century Studios, Ingenious Media, Lightstorm Entertainment\n",
      "instance of:3D film, live-action/animated film\n",
      "Australian Classification:PG, M\n",
      "director of photography:Mauro Fiore\n",
      "IMDb ID:tt0499549\n",
      "original language of film or TV show:English, Na'vi\n",
      "original film format:35 mm film\n",
      "language of work or name:English\n",
      "distribution format:theatrical release, Blu-ray Disc, DVD, video on demand\n",
      "color:color\n",
      "country of origin:United States of America\n",
      "director:James Cameron\n",
      "publication date:2009-12-16\n",
      "screenwriter:James Cameron\n",
      "distributed by:InterCom, Disney+\n",
      "form of creative work:soundtrack\n",
      "filming location:New Zealand, Hawaii, California\n",
      "main subject:cloning, extraterrestrial life, telepresence, ecology\n",
      "POSTING RESPONE:  Avatar was released on December 16, 2009. \n",
      "Took: 9.707409980999728 seconds,avg: 9.316465224500007\n"
     ]
    },
    {
     "data": {
      "text/plain": "' Avatar was released on December 16, 2009. '"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.ask(\"When was Avatar released?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:17:10.186142100Z",
     "start_time": "2023-11-25T13:17:00.463933300Z"
    }
   },
   "id": "548dba55510fc8b2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:17:10.254171100Z",
     "start_time": "2023-11-25T13:17:10.191279400Z"
    }
   },
   "id": "f51749fc7cf9565f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
